{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from copy import copy\n",
    "from candas.learn import GP_gpflow, LVMOGP_GP\n",
    "from candas.learn import ParameterSet\n",
    "import tensorflow as tf\n",
    "from plotting_scripts import contourf_uparray, fit_lmc_from_hps, fit_avg_from_hps, fit_lvmogp_from_hps, \\\n",
    "        plot_all_surfaces, get_test_points, get_prediction, plot_surface, make_axes_locatable\n",
    "# from candas.plotting.regression_plots import plot_uparray, contourf_uparray, parray, Standardizer\n",
    "from candas.style import futura\n",
    "from gpflow.utilities import read_values\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import candas\n",
    "mpl.style.use('mystyle.mplstyle')\n",
    "\n",
    "inch_conversion = 3.93701/100\n",
    "page_width = 142.4 *inch_conversion\n",
    "column_width = 67.2* inch_conversion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Plots of the Surfaces in Bayesian Optimisation Run\n",
    "\n",
    "In this notebook, we make plots of the surfaces for a given test scenario, for a given parameter at a certain iteration.\n",
    "This is to help us visualise what is going on with each of the models at different iterations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specify the Test Case We Want to Plot\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "learning_surface = 'FP005-FP004-EvaGreen' #'FP002-RP002x-EvaGreen' #'FP001-RP001x-Probe'  #'FP001-RP001x-EvaGreen' #'FP001-RP001x-Probe' #'FP001-RP001x-Probe' # 'FP001-RP001x-EvaGreen' #\n",
    "\n",
    "surface_name = 'one_from_many' # 'many' #'many' #'many_both_0_point_start'\n",
    "start_point_name = '0_point_start' #'0_point_start' #'centre'\n",
    "\n",
    "if surface_name == 'one_from_many':\n",
    "    learning_surface_ = f'{learning_surface}'\n",
    "    test_name = f'{surface_name}_{learning_surface_}_both_{start_point_name}'\n",
    "else:\n",
    "    learning_surface_ = ''\n",
    "    test_name = f'{surface_name}_both_{learning_surface_}{start_point_name}'\n",
    "param_name = 'r'\n",
    "#'worst_point_' #worst_point' #'worst_point' # ''\n",
    "# test_name = f'{surface_name}{learning_surface}_both{start_point_name}'\n",
    "# test_name = f'{surface_name}_both_{learning_surface_}{start_point_name}'\n",
    "log_t = 'No_Transform'\n",
    "param = 'r'\n",
    "iteration = 1\n",
    "seed = 2\n",
    "MAP=False\n",
    "log_transform = False\n",
    "save=False\n",
    "restarts = 10\n",
    "restarts_extra = '10_2208_constrained'\n",
    "params = ['r', 'm']\n",
    "random_if_none=True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model_names = [ 'mo_indi', 'avg',  'lmc', 'lvm'] # 'mo_indi','lmc',"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "hyp_dict = {mod_name:{'r': None, 'm':None} for mod_name in model_names}\n",
    "param_dict = {mod_name:{'r': None, 'm':None} for mod_name in model_names}\n",
    "\n",
    "for mod_name in model_names:\n",
    "    for param_name in params:\n",
    "        path = pl.Path.home()/f'Hyperparameters/restarts_{restarts_extra}'\n",
    "        with open(path / (f'hyperparameters_{mod_name}_{test_name}_{random_if_none}_{param_name}_iteration_{iteration}_seed_{seed}_{restarts}.pkl'), \"rb\") as file:\n",
    "                hyper_params = pickle.load(file)\n",
    "        hypparams = hyper_params['hyperparameters'][0]\n",
    "        param_set = hyper_params['parameter array'][0]\n",
    "        hyp_dict[mod_name][param_name] = hypparams\n",
    "        param_dict[mod_name][param_name] = param_set\n",
    "\n",
    "        if not log_transform:\n",
    "            param_set.stdzr.transforms['r'] = [candas.utils.skip, candas.utils.skip]\n",
    "            param_set.stdzr.transforms['m'] = [candas.utils.skip, candas.utils.skip]\n",
    "            for param in ['r', 'm']:\n",
    "                param_set.stdzr[param] = {'μ': param_set.data.loc[(param_set.data['Parameter'] == param) & (param_set.data['Metric'] == 'mean'), 'Value'].mean(),\n",
    "                                   'σ': param_set.data.loc[(param_set.data['Parameter'] == param) & (param_set.data['Metric'] == 'mean'), 'Value'].std()}\n",
    "        else:\n",
    "            pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit the average GP from the hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Parameter: dtype=float64, shape=[], fn=\"chain_of_shift_of_softplus\", numpy=0.09982905982905983>\n",
      "<Parameter: dtype=float64, shape=[], fn=\"chain_of_shift_of_softplus\", numpy=0.09982905982905983>\n",
      "<Parameter: dtype=float64, shape=[], fn=\"chain_of_shift_of_softplus\", numpy=0.09982905982905983>\n",
      "<Parameter: dtype=float64, shape=[], fn=\"chain_of_shift_of_softplus\", numpy=0.09982905982905983>\n",
      "<Parameter: dtype=float64, shape=[], fn=\"chain_of_shift_of_softplus\", numpy=0.09982905982905983>\n",
      "<Parameter: dtype=float64, shape=[], fn=\"chain_of_shift_of_softplus\", numpy=0.09982905982905983>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 16:04:56.527441: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "gp_dict = {mod_name:{'r': None, 'm':None} for mod_name in model_names}\n",
    "for mod_name in model_names:\n",
    "    for param_name in params:\n",
    "        if mod_name == 'avg':\n",
    "            gp = fit_avg_from_hps(param_dict[mod_name][param_name], hyp_dict[mod_name][param_name], param_name)\n",
    "        elif mod_name in ['mo_indi', 'lmc']:\n",
    "            gp = fit_lmc_from_hps(param_dict[mod_name][param_name], hyp_dict[mod_name][param_name], param_name)\n",
    "        else:\n",
    "            gp = fit_lvmogp_from_hps(param_dict[mod_name][param_name], hyp_dict[mod_name][param_name], param_name)\n",
    "        gp_dict[mod_name][param_name] = gp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from gpflow.base import default_float\n",
    "\n",
    "def weighted_mean(means, variances):\n",
    "    \"\"\"calculate the weighted mean\n",
    "    :param means: the means of the values\n",
    "    :param variances: the variances of the values\n",
    "    :return: the weighted mean\"\"\"\n",
    "    weighted_avg = np.sum(means / variances, axis=0) / np.sum(1 / variances, axis=0)\n",
    "    return np.expand_dims(weighted_avg, axis=0)\n",
    "\n",
    "def weighted_variance(means, variances):\n",
    "    \"\"\"calculate the weighted variances\n",
    "    :param means: the means of the values\n",
    "    :param variances: the variances of the values\n",
    "    :return: the weighted variance\"\"\"\n",
    "    weighted_var = np.var(means, axis=0) + np.mean(variances, axis=0)\n",
    "    return np.expand_dims(weighted_var, axis=0)\n",
    "\n",
    "\n",
    "def load_targets():\n",
    "    \"\"\"function to load the target values for each of the surfaces\n",
    "    :returns targets: a dataframe of the target values\"\"\"\n",
    "\n",
    "    path = pl.Path(os.getcwd())\n",
    "    with open(path / 'data' / 'JG067 sequence targets.csv', \"rb\") as file:\n",
    "        targets = pd.read_csv(file)\n",
    "    targets['PrimerPair'] = targets[['FPrimer', 'RPrimer']].agg('-'.join, axis=1)\n",
    "    targets['EvaGreen'] = ((targets['-Strand Label'] == \"None\") & (targets['+Strand Label'] == \"None\"))\n",
    "    targets.loc[targets['EvaGreen'] == True, 'EvaGreen'] = 'EvaGreen'\n",
    "    targets.loc[targets['EvaGreen'] == False, 'EvaGreen'] = 'Probe'\n",
    "    targets['PrimerPairReporter'] = targets[['PrimerPair', 'EvaGreen']].agg('-'.join, axis=1)\n",
    "    targets = targets.drop_duplicates(subset=['PrimerPairReporter'], keep='first')\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "def create_unseen_levels(models, params, targets, reporters=False, primers=False):\n",
    "    \"\"\"get latent variables for the surfaces with no observed data for the LVMOGP. This is done by taking a weighted\n",
    "    average of the latent variables of the surfaces with the same reporter and primer pair (dependent on the bool\n",
    "    of those two parameters)\n",
    "    :param targets: the targets to be used\n",
    "    :param reporters: whether to use reporters\n",
    "    :param primers: whether to use primers\"\"\"\n",
    "\n",
    "    if primers:\n",
    "        reporters = True\n",
    "\n",
    "    for param in params:\n",
    "        lvm = models['lvm'][param]\n",
    "        len_coreg = len(lvm.coregion_coords['PrimerPairReporter'])\n",
    "        j = 0\n",
    "        methods = {}\n",
    "        same_reporter_dict = {}\n",
    "        if reporters:\n",
    "            for reporter in targets['EvaGreen'].unique():\n",
    "                same_reporter_dict[reporter] = lvm.data.data.loc[\n",
    "                    lvm.data.data['EvaGreen'] == reporter, 'PrimerPairReporter'].unique()\n",
    "\n",
    "        if primers:\n",
    "            same_P_dict = {}\n",
    "\n",
    "            for FP in np.unique(targets['FPrimer'].tolist() + targets['RPrimer'].tolist()):\n",
    "                same_P_dict[FP] = np.unique(\n",
    "                    lvm.data.data.loc[lvm.data.data['FPrimer'] == FP, 'PrimerPairReporter'].tolist() + \\\n",
    "                    lvm.data.data.loc[lvm.data.data['RPrimer'] == FP, 'PrimerPairReporter'].tolist())\n",
    "\n",
    "        for i, targ in enumerate(targets['PrimerPairReporter'].unique()):\n",
    "            method = 'observed data'\n",
    "            if targ not in list(lvm.coregion_coords['PrimerPairReporter'].keys()):\n",
    "                lvm.coregion_coords['PrimerPairReporter'][targ] = len_coreg + j\n",
    "                if reporters:\n",
    "                    reporter = targets.loc[targets['PrimerPairReporter'] == targ, 'EvaGreen'].iloc[0]\n",
    "                    if primers:\n",
    "                        FPrimer = targets.loc[targets['PrimerPairReporter'] == targ, 'FPrimer'].iloc[0]\n",
    "                        RPrimer = targets.loc[targets['PrimerPairReporter'] == targ, 'RPrimer'].iloc[0]\n",
    "                        same_Primer = same_P_dict[FPrimer].tolist() + same_P_dict[RPrimer].tolist()\n",
    "                        # same_Primer = same_RP_dict[RPrimer].tolist() + same_FP_dict[FPrimer].tolist()\n",
    "                        same_primer_and_reporter = list(set(same_reporter_dict[reporter]).intersection(same_Primer))\n",
    "                        if len(same_primer_and_reporter) < 2:\n",
    "                            same_primer_and_reporter = same_reporter_dict[reporter]\n",
    "                            method = 'reporter'\n",
    "                        method = \"reporter + primers\"\n",
    "                        if len(list(set(same_reporter_dict[reporter]).intersection(\n",
    "                                same_P_dict[FPrimer].tolist()))) == 0 or \\\n",
    "                                len(list(set(same_reporter_dict[reporter]).intersection(\n",
    "                                    same_P_dict[RPrimer].tolist()))) == 0:\n",
    "                            same_primer_and_reporter = same_reporter_dict[reporter]\n",
    "                            method = 'reporter'\n",
    "                    else:\n",
    "                        same_primer_and_reporter = same_reporter_dict[reporter]\n",
    "                        method = 'reporter'\n",
    "\n",
    "                    same_reporter_index = [lvm.coregion_coords['PrimerPairReporter'][rep]\n",
    "                                           for rep in same_primer_and_reporter]\n",
    "\n",
    "                    H_means = copy(lvm.model.H_data_mean).numpy()[same_reporter_index, :]\n",
    "                    H_vars = copy(lvm.model.H_data_var).numpy()[same_reporter_index, :]\n",
    "                    H_mean = tf.convert_to_tensor(weighted_mean(H_means, H_vars), dtype=default_float())\n",
    "                    H_var = tf.convert_to_tensor(weighted_variance(H_means, H_vars), dtype=default_float())\n",
    "                else:\n",
    "                    H_mean = tf.zeros((1, lvm.model.H_data_mean.shape[1]), dtype=default_float())\n",
    "                    H_var = tf.ones((1, lvm.model.H_data_mean.shape[1]), dtype=default_float())\n",
    "                    method = 'prior'\n",
    "\n",
    "                lvm.model.H_data_mean = tf.concat([lvm.model.H_data_mean, H_mean], axis=0)\n",
    "                lvm.model.H_data_var = tf.concat([lvm.model.H_data_var, H_var], axis=0)\n",
    "                j = j + 1\n",
    "            methods[targ] = method\n",
    "        models['lvm'][param] = lvm\n",
    "    return models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "if (iteration == 0) & (start_point_name == '0_point_start'):\n",
    "    targets = load_targets()\n",
    "    create_unseen_levels(gp_dict, params, targets, reporters=True, primers=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ruby/Transfer_Learning_Design_of_Experiments_for_DNA_Optimisation/analysis/data/ADVI_ParameterSets_220528.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod_name \u001B[38;5;129;01min\u001B[39;00m model_names:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m param_name \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[0;32m----> 7\u001B[0m         test_locations \u001B[38;5;241m=\u001B[39m \u001B[43mget_test_points\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetcwd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmod_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mparam_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m learning_surface \u001B[38;5;129;01min\u001B[39;00m test_locations[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrimerPairReporter\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munique():\n\u001B[1;32m      9\u001B[0m             test_points \u001B[38;5;241m=\u001B[39m test_locations[test_locations[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrimerPairReporter\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m learning_surface][[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBP\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGC\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "File \u001B[0;32m~/Transfer_Learning_Design_of_Experiments_for_DNA_Optimisation/analysis/plotting_scripts.py:89\u001B[0m, in \u001B[0;36mget_test_points\u001B[0;34m(path, param_set)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_test_points\u001B[39m(path, param_set):\n\u001B[1;32m     85\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get the test data point locations from the data set\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03m    :param path: path to the data set\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m    :param param_set: parameter set of the training data\"\"\"\u001B[39;00m\n\u001B[0;32m---> 89\u001B[0m     ps_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_pickle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mADVI_ParameterSets_220528.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m     ps_df \u001B[38;5;241m=\u001B[39m ps_df[(ps_df\u001B[38;5;241m.\u001B[39mlg10_Copies \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m8\u001B[39m)]\n\u001B[1;32m     91\u001B[0m     ps_df \u001B[38;5;241m=\u001B[39m ps_df\u001B[38;5;241m.\u001B[39mdrop(ps_df[ps_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExperiment\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mcontains(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJG073A\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/anaconda3/envs/tl_doe/lib/python3.9/site-packages/pandas/io/pickle.py:196\u001B[0m, in \u001B[0;36mread_pickle\u001B[0;34m(filepath_or_buffer, compression, storage_options)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;124;03m>>> os.remove(\"./dummy.pkl\")\u001B[39;00m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    195\u001B[0m excs_to_catch \u001B[38;5;241m=\u001B[39m (\u001B[38;5;167;01mAttributeError\u001B[39;00m, \u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m)\n\u001B[0;32m--> 196\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[1;32m    203\u001B[0m \n\u001B[1;32m    204\u001B[0m     \u001B[38;5;66;03m# 1) try standard library Pickle\u001B[39;00m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001B[39;00m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001B[39;00m\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001B[39;00m\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/tl_doe/lib/python3.9/site-packages/pandas/io/common.py:711\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    702\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    703\u001B[0m             handle,\n\u001B[1;32m    704\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    707\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    708\u001B[0m         )\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    710\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 711\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    712\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    714\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/ruby/Transfer_Learning_Design_of_Experiments_for_DNA_Optimisation/analysis/data/ADVI_ParameterSets_220528.pkl'"
     ]
    }
   ],
   "source": [
    "xy_pas_dict = {mod_name:{'r': None, 'm':None} for mod_name in model_names}\n",
    "z_upas_dict = {mod_name:{'r': None, 'm':None} for mod_name in model_names}\n",
    "pp_dict = {mod_name:{'r': None, 'm':None} for mod_name in model_names}\n",
    "test_locations_dict = {mod_name:{'r': None, 'm':None} for mod_name in model_names}\n",
    "for mod_name in model_names:\n",
    "    for param_name in params:\n",
    "        test_locations = get_test_points(pl.Path(os.getcwd()), param_dict[mod_name][param_name])\n",
    "        if learning_surface in test_locations['PrimerPairReporter'].unique():\n",
    "            test_points = test_locations[test_locations['PrimerPairReporter'] == learning_surface][['BP', 'GC']]\n",
    "            test_points = gp_dict[mod_name][param_name].parray(**{name: test_points[name] for name in test_points.columns})\n",
    "        else:\n",
    "            test_points = None\n",
    "        test_locations_dict[mod_name][param_name] = test_points\n",
    "\n",
    "        if mod_name == 'avg':\n",
    "            pprs = 0\n",
    "        else:\n",
    "\n",
    "            pprs = gp_dict[mod_name][param_name].coregion_coords['PrimerPairReporter']\n",
    "\n",
    "        xy_pas, z_upas, pp = get_prediction(gp_dict[mod_name][param_name],\n",
    "                                            mod_name, learning_surface, pprs, {}, {})\n",
    "        xy_pas_dict[mod_name][param_name] = xy_pas\n",
    "        z_upas_dict[mod_name][param_name] = z_upas\n",
    "        pp_dict[mod_name][param_name] = pp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the expected improvements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def create_target_parrays(ppr, model, params, targets):\n",
    "    \"\"\"create parrays for each parameter for the target point\n",
    "    :param i: the index of the target point\n",
    "    :param model: the model \"\"\"\n",
    "    target_parrays = {}\n",
    "    for param in params:\n",
    "        if param == 'r':\n",
    "            target_parrays[param] = model['r'].parray(**{'r': targets[targets['PrimerPairReporter']==ppr]['Target Rate']})\n",
    "        elif param == 'm':\n",
    "            target_parrays[param] = model['m'].parray(**{'m': 1e-2}, stdzd=False)\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(f'param {param} isn\\'t r or m')\n",
    "    return target_parrays"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "targets = load_targets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from experiments.expected_improvements import ExpectedImprovementConstrained\n",
    "\n",
    "EI = ExpectedImprovementConstrained(params)\n",
    "eis_dict = {mod_name: {} for mod_name in model_names}\n",
    "for mod_name in ['mo_indi', 'avg', 'lmc', 'lvm']:\n",
    "    model = gp_dict[mod_name]\n",
    "    target_parrays = create_target_parrays(learning_surface, model, params, targets)\n",
    "    ys = {}\n",
    "    ys_parray = {}\n",
    "    preds = {}\n",
    "    for param in params:\n",
    "        ys_ = param_dict[mod_name][param].data[\n",
    "            (param_dict[mod_name][param].data['Parameter']\n",
    "             == param) & (param_dict[mod_name][param].data[\n",
    "                              'PrimerPairReporter'] == learning_surface)\n",
    "            & (param_dict[mod_name][param].data[\n",
    "                   'Metric'] == 'mean')]\n",
    "        ys[param] = ys_.groupby(['BP', 'GC', 'PrimerPairReporter']).mean().reset_index()['Value']\n",
    "        ys_parray[param] = model[param].parray(**{param: ys[param]})\n",
    "        preds[f'{param}_mu_z'] = z_upas_dict[mod_name][param][learning_surface].z.μ.ravel()\n",
    "        preds[f'{param}_sig2_z'] = z_upas_dict[mod_name][param][learning_surface].z.σ2.ravel()\n",
    "\n",
    "    if (iteration == 0) & (start_point_name == '0_point_start'):\n",
    "        best_yet = np.ones(1, ) * 4\n",
    "    else:\n",
    "        best_yet = EI.BestYet({param: ys_parray[param].z.values() for param in  params},\n",
    "                                                   target_parrays)\n",
    "    mu =  np.atleast_2d(preds[f'r_mu_z']).T\n",
    "    sig2 = np.atleast_2d(preds[f'r_sig2_z']).T\n",
    "    target = np.atleast_2d(target_parrays['r'].z.values())\n",
    "\n",
    "    mu_m = np.atleast_2d(preds[f'm_mu_z']).T\n",
    "    sig2_m =  np.atleast_2d(preds[f'm_sig2_z']).T\n",
    "    threshold_m =  np.atleast_2d(target_parrays['m'].z.values())\n",
    "\n",
    "    k = 1\n",
    "    chi_ei = EI.Chi_EI(mu, sig2, target, best_yet, k=k)\n",
    "    ef = EI.expected_feasibility(mu_m.flatten(), sig2_m.flatten(), threshold_m.flatten())\n",
    "    ei = chi_ei.flatten() * ef.flatten()\n",
    "    eis_dict[mod_name][learning_surface] = ei.reshape(100, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(f'best_df_constrained.pkl', \"rb\") as file:\n",
    "    best_df = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the predictions of the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from candas.learn import ParameterArray, UncertainParameterArray, ParameterSet, Standardizer\n",
    "from candas.learn import parray, uparray\n",
    "\n",
    "def unstandardize_axis_labels(ax, axis, array):\n",
    "    if axis == 'x':\n",
    "        ticks = ax.get_xticks()\n",
    "        set_labels = ax.set_xticklabels\n",
    "    elif axis == 'y':\n",
    "        ticks = ax.get_yticks()\n",
    "        set_labels = ax.set_yticklabels\n",
    "    elif axis == 'z':\n",
    "        ticks = ax.get_zticks()\n",
    "        set_labels = ax.set_zticklabels\n",
    "\n",
    "    if isinstance(array, ParameterArray):\n",
    "        name = array.names[0]\n",
    "        stdzr = array.stdzr\n",
    "    elif isinstance(array, UncertainParameterArray):\n",
    "        name = array.name\n",
    "        stdzr = array.dstdzr.stdzr\n",
    "    else:\n",
    "        raise TypeError('Argument \"array\" must be either a ParameterArray or an UncertainParameterArray.')\n",
    "\n",
    "    ticks = parray(**{name: ticks}, stdzr=stdzr, stdzd=True)\n",
    "    set_labels(ticks.values())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, axs_ = plt.subplots(nrows=4, ncols=len(params)*2 +1, figsize=(page_width*2, 6))\n",
    "\n",
    "xy_pas = {}\n",
    "z_upas = {}\n",
    "\n",
    "limits = {'r': {'mu':[0, 1.2], 'sig': [0.04, 0.7]},\n",
    "          'm':{'mu':None,  'sig': None}}\n",
    "full_width = 5.5984252\n",
    "page_height = 7.85\n",
    "halfwidth = 2.645669\n",
    "\n",
    "for j, mod_name in enumerate(model_names):\n",
    "\n",
    "    for i, param_name in enumerate(params):\n",
    "        locations = param_dict[mod_name][param_name].data[['BP', 'GC', 'PrimerPairReporter']]\n",
    "        plot_surface(xy_pas_dict[mod_name][param_name], z_upas_dict[mod_name][param_name],\n",
    "                     gp_dict[mod_name][param_name], mod_name, learning_surface, pp_dict[mod_name][param_name],\n",
    "                     param_name,None, stdz=False, save=False, limits=limits[param_name],\n",
    "                      axs=axs_[j, i*2:(i+1)*2], input_locations=locations)\n",
    "        # axs_[0].set_title(r'$\\mu$')\n",
    "        # axs_[1].set_title(r'$2\\sigma$')\n",
    "        # axs_[0].grid(False)\n",
    "        # axs_[1].grid(False)\n",
    "    # test = threshold_m.flatten()[0]\n",
    "    # test2  =xy_pas_dict[mod_name][param_name][learning_surface]['BP']\n",
    "    # test3 = z_upas_dict[mod_name][param_name][learning_surface]\n",
    "\n",
    "    CS = axs_[j, 0].contour(*[xy_pas_dict[mod_name]['r'][learning_surface]['BP'].z,\n",
    "                       xy_pas_dict[mod_name]['r'][learning_surface]['GC'].z],\n",
    "                          z_upas_dict[mod_name]['r'][learning_surface].μ,\n",
    "                                 levels=[targets[targets['PrimerPairReporter']\n",
    "                                                == learning_surface]['Target Rate'].to_numpy()[0]], colors='k')\n",
    "    axs_[j, 0].clabel(CS, inline=True, fontsize=8)\n",
    "\n",
    "    axs_[j, 0].annotate(mod_name, xy=(-50, 25), rotation=90, xycoords='axes pixels')\n",
    "\n",
    "\n",
    "    CS = axs_[j, 2].contour(*[xy_pas_dict[mod_name]['m'][learning_surface]['BP'].z,\n",
    "                       xy_pas_dict[mod_name]['m'][learning_surface]['GC'].z],\n",
    "                          z_upas_dict[mod_name]['m'][learning_surface].μ,\n",
    "                                 levels=[0.01], colors='k')\n",
    "\n",
    "    axs_[j, 2].clabel(CS, inline=True, fontsize=8)\n",
    "\n",
    "\n",
    "    # ax.clabel(CS, inline=True, fontsize=10)\n",
    "\n",
    "    print(eis_dict[mod_name][learning_surface].shape)\n",
    "    defaults = dict(levels=16, cmap='viridis')\n",
    "    contour = axs_[j, -1].contourf(*[xy_pas_dict[mod_name][param_name][learning_surface]['BP'].z,\n",
    "                       xy_pas_dict[mod_name][param_name][learning_surface]['GC'].z],\n",
    "                          eis_dict[mod_name][learning_surface], **{**defaults})\n",
    "    input_locations = param_dict[mod_name][param_name].data[['BP', 'GC', 'PrimerPairReporter']]\n",
    "    input_locations = input_locations[input_locations['PrimerPairReporter'] == learning_surface]\n",
    "    Xs = gp_dict[mod_name]['r'].parray(**{'BP': input_locations['BP'], 'GC': input_locations['GC']})\n",
    "    Xs = np.squeeze(np.hstack([np.atleast_2d(Xs['BP'].z.values()).T,\n",
    "                                   np.atleast_2d(Xs['GC'].z.values()).T]))\n",
    "    Xs = Xs.reshape(len(Xs), 2)\n",
    "    axs_[j, -1].scatter(Xs[:, 0], Xs[:, 1], marker='o', color='k', zorder=10)\n",
    "\n",
    "    #\n",
    "    # axs_[j, 4].scatter(xy_pas_dict[mod_name][param_name][learning_surface]['BP'].z.values(),\n",
    "    #                                                 xy_pas_dict[mod_name][param_name][learning_surface]['GC'].z.values(),\n",
    "    #                    marker='o', facecolors='none', linewidth=1.5, alpha=0.5, color='white', zorder=6)\n",
    "    # cbar = plt.colorbar(cont, ax=axs[i], cax=cax1, format='%.2g')\n",
    "    cbar = plt.colorbar(contour, ax=axs_[j, -1] ,format='%.2g')\n",
    "\n",
    "    best_point = best_df[best_df['PrimerPairReporter'] == learning_surface]\n",
    "\n",
    "    best_point_parray = gp_dict[mod_name][param].parray(**{'GC': best_point['GC'], 'BP': best_point['BP']})\n",
    "\n",
    "    for ax in axs_.flatten():\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title('')\n",
    "        ax.grid(False)\n",
    "        # ax.scatter(best_point_parray['BP'].z.values(),best_point_parray['GC'].z.values(),\n",
    "        #                marker='o', facecolors='none', linewidth=1.5, alpha=0.5, color='r', zorder=6)\n",
    "        # ax.scatter(test_locations_dict[mod_name][param_name]['BP'].z.values(),\n",
    "        #                                             test_locations_dict[mod_name][param_name]['GC'].z.values(),\n",
    "        #                marker='o', facecolors='none', linewidth=1.5, alpha=0.5, color='white', zorder=5)\n",
    "\n",
    "\n",
    "    for ax in axs_[:, -1].flatten():\n",
    "        ax.set_xticklabels([int(float(txt.get_text())) for txt in axs_[0, 0].get_xticklabels()])\n",
    "        ax.set_yticklabels([int(float(txt.get_text())) for txt in axs_[0, 0].get_yticklabels()])\n",
    "        ax.set_xlabel('BP')\n",
    "        ax.set_ylabel('GC')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.4)\n",
    "\n",
    "plt.savefig(f'plots/preds_and_ei_iteration_{learning_surface}_{iteration}_seed_{seed}_restarts_{restarts}.svg', bbox_inches='tight')\n",
    "plt.savefig(f'plots/preds_and_ei_iteration_{learning_surface}_{iteration}_seed_{seed}_restarts_{restarts}.png', bbox_inches='tight', dpi=2000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "fig, axs_ = plt.subplots(nrows=4, ncols=5, figsize=(page_width*2, 6))\n",
    "\n",
    "xy_pas = {}\n",
    "z_upas = {}\n",
    "\n",
    "limits = {'r': {'mu':[0.15, 1.2], 'sig': [0.04, 0.55]},\n",
    "          'm':{'mu':None,  'sig': None}}\n",
    "full_width = 5.5984252\n",
    "page_height = 7.85\n",
    "halfwidth = 2.645669\n",
    "\n",
    "for j, mod_name in enumerate(model_names):\n",
    "    for i, param_name in enumerate(['r', 'm']):\n",
    "        locations = param_dict[mod_name][param_name].data[['BP', 'GC', 'PrimerPairReporter']]\n",
    "\n",
    "        pp_name = learning_surface\n",
    "        axs = axs_[j, i*2:(i+1)*2]\n",
    "        model_name = mod_name\n",
    "        input_locations = locations\n",
    "\n",
    "        model = gp_dict[model_name][param_name]\n",
    "        test_points = test_locations_dict[model_name][param_name]\n",
    "\n",
    "        xy_pa = xy_pas_dict[mod_name][param_name][pp_name]\n",
    "        z_upa = z_upas_dict[mod_name][param_name][pp_name]\n",
    "\n",
    "        if model_name in ['lmc', 'mo_indi']:\n",
    "            indices = np.argwhere(model.X['PrimerPairReporter'].z.values() == pp)[:, 0]\n",
    "            Xs = np.squeeze(np.hstack([model.X['BP'].values()[indices],\n",
    "                                       model.X['GC'].values()[indices]]))\n",
    "            Xs = Xs.reshape(len(indices), 2)\n",
    "        elif model_name == 'avg':\n",
    "            locs = input_locations[input_locations['PrimerPairReporter'] == pp_name]\n",
    "            Xs = model.parray(**{'BP': locs['BP'], 'GC': locs['GC']})\n",
    "            Xs = np.squeeze(np.hstack([np.atleast_2d(Xs['BP'].z.values()).T,\n",
    "                                       np.atleast_2d(Xs['GC'].z.values()).T]))\n",
    "            # Xs = np.squeeze(model.model.data[0].numpy())\n",
    "            Xs = Xs.reshape(len(Xs), 2)\n",
    "        else:\n",
    "            indices = np.argwhere(model.model.X_data_fn.numpy() == pp)\n",
    "            Xs = np.squeeze(model.model.X_data.numpy()[indices])\n",
    "            Xs = Xs.reshape(len(indices), 2)\n",
    "\n",
    "        axs[0].scatter(Xs[:, 0], Xs[:, 1], marker='o', color='r', zorder=5)\n",
    "        axs[1].scatter(Xs[:, 0], Xs[:, 1], marker='o', color='r', zorder=5)\n",
    "        axs[0].set_xlim(xy_pa['BP'].z.values().min(),xy_pa['BP'].z.values().max() )\n",
    "        axs[0].set_ylim(xy_pa['GC'].z.values().min(),xy_pa['GC'].z.values().max() )\n",
    "        axs[1].set_xlim(xy_pa['BP'].z.values().min(),xy_pa['BP'].z.values().max() )\n",
    "        axs[1].set_ylim(xy_pa['GC'].z.values().min(),xy_pa['GC'].z.values().max() )\n",
    "        axs[0].scatter(test_points['BP'].z.values(), test_points['GC'].z.values(),\n",
    "                       marker='o', facecolors='none', linewidth=1.5, alpha=0.5, color='k', zorder=5)\n",
    "        axs[1].scatter(test_points['BP'].z.values(), test_points['GC'].z.values(),\n",
    "                           marker='o', facecolors='none', linewidth=1.5, alpha=0.5, color='k', zorder=5)\n",
    "\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.savefig(f'plots/preds_and_ei_iteration_{iteration}_seed_{seed}_restarts_{restarts}.svg', bbox_inches='tight')\n",
    "# plt.savefig(f'plots/preds_and_ei_iteration_{iteration}_seed_{seed}_restarts_{restarts}.png', bbox_inches='tight', dpi=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = z_upas_dict['avg']['r'][learning_surface].z.μ.flatten()\n",
    "print(max(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the Latent Coordinates of the LVMOGP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "pprs = gp_dict['lvm']['r'].coregion_coords['PrimerPairReporter'].keys()\n",
    "pprs_probes = list(filter(lambda ppr: 'Probe' in ppr, pprs))\n",
    "pprs_evagreen = list(filter(lambda ppr: 'EvaGreen' in ppr, pprs))\n",
    "pprs = pprs_probes + pprs_evagreen\n",
    "print(pprs)\n",
    "for param, hyper_params in hyp_dict['lvm'].items():\n",
    "    H_mean = hyper_params['.H_data_mean']\n",
    "    H_var = hyper_params['.H_data_var']\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']*10\n",
    "    fig = plt.figure(figsize=(column_width, column_width)); ax = plt.gca()\n",
    "    # for i in range(len(H_mean)):\n",
    "    for ppr in pprs:\n",
    "        i = gp_dict['lvm']['r'].coregion_coords['PrimerPairReporter'][ppr]\n",
    "        # for key, value in gp_dict['lvm']['r'].coregion_coords['PrimerPairReporter'].items():\n",
    "        #     if value == i:\n",
    "        #         ppr = key\n",
    "        if 'EvaGreen' in ppr:\n",
    "            marker='o'\n",
    "\n",
    "        else:\n",
    "            marker = 'x'\n",
    "\n",
    "        ax.scatter(H_mean[i, 0], H_mean[i, 1], label=f'{ppr}',\n",
    "                   color=colors[i], marker=marker, s=50)\n",
    "        # ax.annotate(f'{list(model.coregion_coords[\"PrimerPairReporter\"].keys())[i]}', (h[dims[0]], h[dims[1]]), fontsize=10)\n",
    "        ax.set_xlabel(f'Latent 1')\n",
    "        ax.set_ylabel(f'Latent 2')\n",
    "        # ax.set_title('LVM Latent Variable Coordinates')\n",
    "        circle1 = Ellipse((H_mean[i, 0], H_mean[i, 1]), 1.95 * np.sqrt(H_var[i, 0]),\n",
    "                          1.95 * np.sqrt(H_var[i, 1]), color=colors[i], alpha=0.15, zorder=0)\n",
    "        ax.add_patch(circle1)\n",
    "        # ax.legend(loc='center left', ncol=2, bbox_to_anchor=(1, 0.5))\n",
    "    if param == 'r':\n",
    "        ax.set_xlim(-2, 3)\n",
    "        ax.set_ylim(-2, 3)\n",
    "    if param == 'm':\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_yticks([-1, 0, 1])\n",
    "\n",
    "    path = pl.Path.home()/'surface_plots'\n",
    "    plt.savefig(path /f'LVM_latents_{surface_name}_{iteration}_{test_name}_{seed}_{param}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(path /f'LVM_latents_{surface_name}_{iteration}_{test_name}_{seed}_{param}.svg', bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "pprs = gp_dict['lvm']['r'].coregion_coords['PrimerPairReporter'].keys()\n",
    "pprs_probes = list(filter(lambda ppr: 'Probe' in ppr, pprs))\n",
    "pprs_evagreen = list(filter(lambda ppr: 'EvaGreen' in ppr, pprs))\n",
    "pprs = pprs_probes + pprs_evagreen\n",
    "print(pprs)\n",
    "for param, hyper_params in hyp_dict['lvm'].items():\n",
    "    H_mean = hyper_params['.H_data_mean']\n",
    "    H_var = hyper_params['.H_data_var']\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']*10\n",
    "    fig = plt.figure(figsize=(0.5*column_width, 0.5*column_width)); ax = plt.gca()\n",
    "    # for i in range(len(H_mean)):\n",
    "    for ppr in pprs:\n",
    "        i = gp_dict['lvm']['r'].coregion_coords['PrimerPairReporter'][ppr]\n",
    "        # for key, value in gp_dict['lvm']['r'].coregion_coords['PrimerPairReporter'].items():\n",
    "        #     if value == i:\n",
    "        #         ppr = key\n",
    "        if 'EvaGreen' in ppr:\n",
    "            marker='o'\n",
    "\n",
    "        else:\n",
    "            marker = 'x'\n",
    "\n",
    "        ax.scatter(H_mean[i, 0], H_mean[i, 1], label=f'{ppr}',\n",
    "                   color=colors[i], marker=marker)\n",
    "        # ax.annotate(f'{list(model.coregion_coords[\"PrimerPairReporter\"].keys())[i]}', (h[dims[0]], h[dims[1]]), fontsize=10)\n",
    "        ax.set_xlabel(f'Latent 1', labelpad=0)\n",
    "        ax.set_ylabel(f'Latent 2', labelpad=0)\n",
    "        # ax.set_title('LVM Latent Variable Coordinates')\n",
    "        circle1 = Ellipse((H_mean[i, 0], H_mean[i, 1]), 1.95 * np.sqrt(H_var[i, 0]),\n",
    "                          1.95 * np.sqrt(H_var[i, 1]), color=colors[i], alpha=0.15, zorder=0)\n",
    "        ax.add_patch(circle1)\n",
    "        # ax.legend(loc='center left', ncol=2, bbox_to_anchor=(1, 0.5))\n",
    "    if param == 'r':\n",
    "        ax.set_xlim(-2, 3)\n",
    "        ax.set_ylim(-2, 3)\n",
    "        ax.set_xticks([-2, -1, 0, 1, 2, 3])\n",
    "        ax.set_yticks([-2, -1, 0, 1, 2, 3])\n",
    "    if param == 'm':\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_xticks([-3, -2, -1, 0, 1, 2, 3])\n",
    "        ax.set_yticks([-1, 0, 1])\n",
    "    # plt.subplots_adjust(hspace=0.01, wspace=0.01, left=None, bottom=0.18, right=None, top=1)\n",
    "    # plt.margins(y=0)\n",
    "    # plt.tight_layout()\n",
    "    path = pl.Path.home()/'surface_plots'\n",
    "    plt.savefig(path /f'LVM_latents_{surface_name}_{iteration}_{test_name}_{seed}_{param}_smaller.pdf', bbox_inches='tight')\n",
    "    plt.savefig(path /f'LVM_latents_{surface_name}_{iteration}_{test_name}_{seed}_{param}_smaller.svg', bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all_ps = fns.load_parameterset(log_t=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\n",
    "ax = axs.flatten()\n",
    "\n",
    "model = gp_dict['avg']['r']\n",
    "\n",
    "\n",
    "for i, ppr in enumerate(best_df['PrimerPairReporter']):\n",
    "\n",
    "    best_df_ppr = best_df[best_df['PrimerPairReporter'] == ppr]\n",
    "    best_point_parray = model.parray(**{'BP': best_df_ppr['BP'], 'GC':best_df_ppr['GC']})\n",
    "\n",
    "    ax[i].scatter(best_point_parray['BP'].z.values(), best_point_parray['GC'].z.values(), color='r', zorder=3)\n",
    "\n",
    "    df = all_ps.data[all_ps.data['PrimerPairReporter'] == ppr].drop_duplicates()\n",
    "    point_parray= model.parray(**{'BP': df['BP'], 'GC':df['GC']})\n",
    "    ax[i].scatter(point_parray['BP'].z.values(), point_parray['GC'].z.values(), color='k')\n",
    "    ax[i].set_title(ppr)\n",
    "    lims_parray = model.parray(**{'BP': np.array([[4], [88]]), 'GC':np.array([[9], [84]])})\n",
    "    ax[i].set_ylim(-2.5, 2.5)\n",
    "    ax[i].set_xlim(-2.5, 2.5)\n",
    "\n",
    "path = pl.Path.home()/'surface_plots'\n",
    "plt.savefig(path /f'standardised_best_points.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\n",
    "ax = axs.flatten()\n",
    "\n",
    "model = gp_dict['avg']['r']\n",
    "\n",
    "\n",
    "for i, ppr in enumerate(best_df['PrimerPairReporter']):\n",
    "\n",
    "    best_df_ppr = best_df[best_df['PrimerPairReporter'] == ppr]\n",
    "    best_point_parray = model.parray(**{'BP': best_df_ppr['BP'], 'GC':best_df_ppr['GC']})\n",
    "\n",
    "    ax[i].scatter(best_point_parray['BP'].values(), best_point_parray['GC'].values(), color='r', zorder=3)\n",
    "\n",
    "    df = all_ps.data[all_ps.data['PrimerPairReporter'] == ppr].drop_duplicates()\n",
    "    point_parray= model.parray(**{'BP': df['BP'], 'GC':df['GC']})\n",
    "    ax[i].scatter(point_parray['BP'].values(), point_parray['GC'].values(), color='k')\n",
    "    ax[i].set_title(ppr)\n",
    "    ax[i].set_xlabel('BP')\n",
    "    ax[i].set_xlabel('GC')\n",
    "    lims_parray = model.parray(**{'BP': np.array([[4], [88]]), 'GC':np.array([[9], [84]])})\n",
    "    ax[i].set_ylim(0, 0.85)\n",
    "    ax[i].set_xlim(10, 590)\n",
    "    ax[i].scatter( 282, 0.360511, color='green')\n",
    "\n",
    "path = pl.Path.home()/'surface_plots'\n",
    "plt.savefig(path /f'unstandardised_best_points.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}